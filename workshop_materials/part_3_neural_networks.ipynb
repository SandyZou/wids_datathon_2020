{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WiDS DATATHON WORKSHOP\n",
    "## FEBRUARY 15, 2020\n",
    "<img src=\"images/inst_logos.png\" alt=\"Harvard IACS\" style=\"height: 80px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Includes the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt \n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import Callback, ModelCheckpoint, History \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, fetch_olivetti_faces\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, scale\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random_state = 0\n",
    "random = np.random.RandomState(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Classification with Non-linear Decision Boundaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification with Linear Boundaries\n",
    "In **logistic regression**, we model the probability of an input $\\mathbf{x}$ being labeled '1' as a function of its distance from the hyperplane parametrized by $\\mathbf{w}$\n",
    "\n",
    "<img src=\"./images/fig0.png\" style='height:300px;'>\n",
    "\n",
    "That is, we model $p(y=1 | \\mathbf{w}, \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$. Where $\\mathbf{w}^\\top \\mathbf{x}=0$ is the equation of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a ellipitical decision boundary?\n",
    "\n",
    "<img src=\"./images/fig1.png\" style='height:300px;'>\n",
    "\n",
    "We can say that the decision boundary is given by a ***quadratic function*** of the input:\n",
    "$$\n",
    "w_1x^2_1 + w_2x^2_2 + w_3 = 0\n",
    "$$\n",
    "We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a complex decision boundary?\n",
    "\n",
    "<img src=\"./images/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Decision Boundaries of Different Classifiers\n",
    "\n",
    "We've proposed three types of models that can capture complex decision boundaries: \n",
    "1. logistic regression with polynomial boundaries\n",
    "2. decision trees\n",
    "3. KNN classifiers\n",
    "\n",
    "<img src=\"./images/fig3.png\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Embarassment of Riches\n",
    "\n",
    "Now we have seen three types of classifiers: logistic regression, decision tree, kNN. Each type can be customized in many ways (e.g. we can choose many different polynomials for the logistic regression boundary)\n",
    "\n",
    "***Question:*** which model should we use?\n",
    "\n",
    "***Answer:*** your choice of model should depend on the task and the dataset. You must\n",
    "1. choose models based on sensible evaluation metrics\n",
    "2. choose models using proper data set splitting procedure (train/validation/test)\n",
    "3. choose models that best solves your real-life task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison of Classifiers: Computational Concerns\n",
    "In addition to considering how well a model solves our real-life task, we also need to consider how easily we can apply a model to large amounts of data - this is called ***scalability***. \n",
    "\n",
    "Specifically, we want to know how easily we can train a model and how easily we can compute a prediction.\n",
    "\n",
    "**QUESTION:** What are the advantages and disadvantages of each type of classifier with respect to scalability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./images/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**REVISED GOAL:** Find models that can capture *arbitrarily complex* decision boundaries **and** are fast to train as well as effecient for computing predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating Arbitrarily Complex Decision Boundaries\n",
    "\n",
    "Given an exact parametrization, we could learn the functional form, $g$, of the decision boundary directly. \n",
    "\n",
    "However, assuming an exact form for $g$ is restrictive. \n",
    "\n",
    "Rather, we can build increasingly good approximations, $\\widehat{g}$, of $g$ by composing simple functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network?\n",
    "\n",
    "**Goal:** build a good approximation $\\widehat{g}$ of a complex function $g$ by composing simple functions.\n",
    "\n",
    "For example, let the following picture represents $f\\left(\\sum_{i}w_ix_i\\right)$, where $f$ is a non-linear transform:\n",
    "\n",
    "<img src=\"./images/fig4.png\" style='height:200px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks as Function Approximators\n",
    "\n",
    "Then we can define the approximation $\\widehat{g}$ with a graphical schema representing a complex series of compositions and sums of the form, $f\\left(\\sum_{i}w_ix_i\\right)$\n",
    "\n",
    "<img src=\"./images/fig5.png\" style='height:300px;'>\n",
    "\n",
    "This is a ***neural network***. We denote the weights of the neural network collectively by $\\mathbf{W}$.\n",
    "The non-linear function $f$ is called the ***activation function***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Translate Graphical Representations into Functional Ones\n",
    "\n",
    "Translate the following graphical representation of a neural network into a functional form, $g(x_1, x_2) = ?$ \n",
    "<img src=\"./images/fig7.png\" style='height:100px;'>\n",
    "Use the following labels:\n",
    "1. the input nodes $x_1$ and $x_2$\n",
    "2. the hidden nodes $h_1$ and $h_2$\n",
    "3. the output node $y$\n",
    "4. the weight from $x_i$ to $h_j$ as $w^i_j$\n",
    "5. the weight from $h_j$ to $y$ as $w^j$\n",
    "6. the activation function $g(x) = e^{-0.5x^2}$\n",
    "\n",
    "**Bonus:** write the functional form in vector notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Flexible Framework for Function Approximation\n",
    "\n",
    "\n",
    "<img src=\"./images/fig6.png\" style='height:500px;'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Choices for the Activation Function\n",
    "\n",
    "<img src=\"./images/fig8.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network for Classification\n",
    "\n",
    "\n",
    "**Data:** features `x_train`, real-valued labels `y_train`\n",
    "\n",
    "**Probabilistic Model:** `y_train` $\\sim \\text{Bernoulli}(\\sigma(g_\\mathbf{W}($ `x_train` $))$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$, and $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the ***binary cross entropy*** or ***log loss***,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{CrossEnt}(\\mathbf{W}) =\\sum^N_{n=1} y_n \\log( g_\\mathbf{W}(x_n)) + (1 - y_n) \\log\\left(1 - g_\\mathbf{W}(x_n)\\right)\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** Typically when we optimize an objective function $\\mathcal{L}$, we comput the gradient of the objective function with respective to the model parameters $\\mathbf{W}$, set it equal to zero and solved for the optimal $\\mathbf{W}$ analytically. \n",
    "\n",
    "Can we analytically optimize $\\mathcal{L}$ when $g_\\mathbf{W}$ is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Optimizing Neural Networks\n",
    "\n",
    "For the small neural network in the previous exercise, compute the gradient $\\nabla_{\\mathbf{W}}\\,\\mathrm{MSE}(\\mathbf{W})$. Can you analytically solve for the optimal parameters $\\mathbf{W}$? Is the training objective convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent for Training Neural Networks: BackProp\n",
    "The intuition behind various flavours of gradient descent  is as follows:\n",
    "\n",
    "<img src=\"./images/fig9.pdf\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent: the Algorithm\n",
    "1. start at random place: $W_0\\leftarrow \\textbf{random}$\n",
    "\n",
    "2. until (stopping condition satisfied):\n",
    "\n",
    "  a. compute gradient: \n",
    "     gradient = $\\nabla$ loss\\_function($W_{t}$)\n",
    "\n",
    "  b. take a step in the negative gradient direction: \n",
    "     $W_{t+1} \\leftarrow W_{t}$ - eta * gradient\n",
    "\n",
    "Here *eta* is called the ***learning rate***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing Neural Networks in `python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `keras`: a Python Library for Neural Networks\n",
    "\n",
    "`keras` is a `python` library that provides intuitive api's for build neural networks quickly. \n",
    "\n",
    "``` python\n",
    "#keras model for feedforward neural networks\n",
    "from keras.models import Sequential\n",
    "\n",
    "#keras model for layers in feedforward networks\n",
    "from keras.layers import Dense\n",
    "\n",
    "#keras model for optimizing training objectives\n",
    "from keras import optimizers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Neural Network for Classification in `keras`\n",
    "\n",
    "``` python\n",
    "#instantiate a feedforward model\n",
    "model = Sequential()\n",
    "\n",
    "#add layers sequentially\n",
    "\n",
    "#input layer: 2 input dimensions\n",
    "model.add(Dense(2, input_dim=2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "#hidden layer: 2 nodes\n",
    "model.add(Dense(2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#output layer: 1 output dimension\n",
    "model.add(Dense(1, activation='sigmoid', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#configure the model: specify training objective and training algorithm\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a Neural Network in `keras`\n",
    "\n",
    "``` python\n",
    "#fit the model and return the mean squared error during training\n",
    "history = model.fit(X_train, Y_train, batch_size=20, shuffle=True, epochs=100, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Your NN: Optimization Choices Matter\n",
    "<img src=\"./images/fig10.jpg\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monitoring Neural Network Training\n",
    "\n",
    "Visualize the mean square error over the training, this is called the training ***trajectory***.\n",
    "\n",
    "``` python\n",
    "#fit the model and return the mean squared error during training\n",
    "history = model.fit(X_train, Y_train, batch_size=20, shuffle=True, epochs=100, verbose=0)\n",
    "\n",
    "# Plot the loss function and the evaluation metric over the course of training\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.array(history.history['mean_squared_error']), color='blue', label='training accuracy')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./images/fig13.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./images/fig14.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./images/fig15.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Train a Neural Network Classifier\n",
    "\n",
    "In the `WiDS_Starter_Notebook_3.ipynb` `colab` notebook, Part II: investigate the effectiveness of a neural network classifier on a toy dataset. \n",
    "\n",
    "Why is the neural network so effective on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Does a Neural Network Learn?\n",
    "\n",
    "<table>\n",
    "    <tr><td><font size=\"3\">A Complex Decision Boundary $g\\quad\\quad\\quad\\quad\\quad$</font></td>\n",
    "        <td><font size=\"3\">A Transformation $g_0$ and a linear model $g_1\\quad\\quad\\quad\\quad$</font></td>\n",
    "    <tr><td><img src=\"images/decision.png\" style=\"height:350px;\"></td>\n",
    "        <td><img src=\"images/architecture2.png\" style=\"height:400px;\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data stratification\n",
    "\n",
    "When the classes in your data set are extremely unbalanced, a random split of your data into training, validation and test may result in one of these sets containing **zero** instances of the rare class.\n",
    "\n",
    "**Problem:** Training on only data containing no rare labels will result in a classifier that is unable to generalize and predict well on data with these labels that may be present in test set.\n",
    "\n",
    "\n",
    "***Stratification*** is the technique to allocate the samples evenly based on sample classes so that training set and validation set have similar ratio of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reweighting classes\n",
    "When classes in your data set are extremely unbalanced, the models you train can be unincentivized to predict correctly on the rare class -- specializing on the overrepresented classes will result in low average loss. \n",
    "\n",
    "We can ***reweight*** the terms in the loss function so that contributions from terms associated with the rare class is increased while the contribution of those associated with the overrepresented classes is decreased. Intuitively, the model is penalized more for being 'wrong' on the rare class.\n",
    "\n",
    "<img src=\"./images/unbalanced.png\" style=\"width: 500px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resampling points\n",
    "\n",
    "We can also create a subset of the data that has a more balanced representation of all classes by: \n",
    "\n",
    "1. Down-sampling (Under sampling) the majority class\n",
    "2. Up-sampling (Over sampling) the minority class\n",
    "3. Fanciers sampling techniques\n",
    "\n",
    "<img src=\"./images/sampling.png\" style=\"width: 500px;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep vs Shallow Trees: The Bias Variance Trade Off\n",
    "If you randomly draw 50 sample from our toy data with non-linear boundary and fit a decision tree on each sample individually, their decision boundaries will look like the following:\n",
    "\n",
    "<img src=\"./images/tree_boundaries.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "\n",
    "What does this say about the variance of very deep trees? How does this impact the generalization of deep trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting\n",
    "Complex models have ***low bias*** -- they can model a wide range of functions, given enough samples.\n",
    "\n",
    "But complex models can use their 'extra' capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have ***high variance*** -- they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.\n",
    "\n",
    "<img src=\"./images/tree_boundary.png\" style=\"width: 500px;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "A way to prevent overfitting is to reduce the capacity of the model, thereby limiting the kinds of functions they can model. This **increases bias, but reduces variance**:\n",
    "1. **$\\ell_1$, $\\ell_2$ weight regularization** - adding a term to the loss function that penalizes the $\\ell_1$-norm (sum of absolute values) or the $\\ell_2$-norm (sum of squares) of the weights. This prevents the network from learning extremely squiggly functions.\n",
    "``` python\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "```\n",
    "\n",
    "2. **Dropout** - randomly zeroing out weights during training. This prevents the hidden nodes from \"over specializing\" or \"memorizing\" certain data points.\n",
    "``` python\n",
    "from keras.layers import Dropout,\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "Another way to prevent overfitting is to create ***ensembles*** of models. ***Bagging*** (Bootstrapping and Aggregating) is an ensemble method that:\n",
    "1. trains a diverse set of complex models on various subsets of the training data -- low bias but high variance\n",
    "2. reduces the variance by averaging the outputs of these models -- random mistakes will 'cancel out' in the average\n",
    "\n",
    "\n",
    "<img src=\"./images/bagging.jpg\" style=\"width: 300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forest Implementation in `sklearn`\n",
    "\n",
    "A **random forest** is the 'averaged model' of collection of deep decision trees each learned on a subset of the training data (each branch in each tree is also trained on a randomized set of input dimensions to reduce correlation between trees).\n",
    "\n",
    "``` python\n",
    "#import the random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#instantiate a forest of with 1000 trees, each of depth 1000\n",
    "forest = RandomForestClassifier(n_estimators=1000, max_depth=1000)\n",
    "\n",
    "#fit your forest to data\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "#predict new labels using the fitted forest\n",
    "forest.predict(x_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## For Practitioners:\n",
    "\n",
    "A random forest, as implemented in `sklearn`, has a number of hyper-parameters that effects model performance. Some important hyper-parameters are:\n",
    "\n",
    "1. number of trees in the forest `n_estimators`\n",
    "2. max depth of trees `max_depth`\n",
    "3. hyper-parameters that determines the subsets of input dimensions to learn each branch and the subsets of training data to learn each tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "***Boosting*** is an ensemble method that builds a complex model iteratively by summing together simple models:\n",
    "1. train a simple model -- high bias but low variance\n",
    "2. train a simple model to focus on the *errors* of the current model, add the new model to the existing model\n",
    "3. repeat\n",
    "\n",
    "***Gradient boosting*** is easy to implement by hand.\n",
    "\n",
    "***Adaboost*** (Adaptive boosting) is implemented by `sklearn`, and can be made to work with neural network models implemented in `keras`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
